{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nithin1807-glitch/Zomato-Restaurant-Analysis-Using-Machine-Learning./blob/main/Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Restaurant Analysis Using Machine Learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Learning & Sentiment Analysis.\n",
        "##### **Contribution**    - Individual.\n",
        "##### **Name -** G.Nithin."
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction\n",
        "Zomato stands as one of India's premier restaurant discovery and food delivery platforms, serving as a bridge between millions of customers and thousands of dining establishments. The platform hosts a vast repository of data, including restaurant metadata (menus, pricing, cuisines) and customer-generated content such as ratings and textual reviews. As the number of restaurants grows, so does the volume of customer feedback, making data analysis essential for understanding performance and consumer preferences.\n",
        "\n",
        "\n",
        "2. The Problem Statement\n",
        "While the abundance of data is an asset, it presents a significant challenge: manual analysis of thousands of reviews and metadata points is time-consuming and inefficient. Customers rely heavily on these reviews to make informed dining choices, yet the raw data is too massive for a human to process manually. This project leverages Machine Learning to automate the extraction of insights from both structured metadata and unstructured customer reviews.\n",
        "\n",
        "\n",
        "3. Methodology & Data Preprocessing\n",
        "The project utilized two primary datasets: a restaurant metadata file containing names, costs, and cuisines, and a reviews dataset containing textual feedback. The preprocessing phase was critical for ensuring data quality. Key steps included:\n",
        "\n",
        "\n",
        "\n",
        "Data Cleaning: Removing non-numeric characters (like commas) from the cost column and handling missing values.\n",
        "\n",
        "\n",
        "Normalization: Applying StandardScaler to the cost features to ensure the K-Means algorithm was not biased by different scales of measurement.\n",
        "\n",
        "\n",
        "NLP Preprocessing: Cleaning textual reviews by removing punctuation and converting text to lowercase to prepare for sentiment analysis.\n",
        "\n",
        "4. Machine Learning Implementation\n",
        "I implemented a dual-model approach to gain a 360-degree view of the data:\n",
        "\n",
        "\n",
        "Unsupervised Learning (K-Means Clustering): Using the Elbow Method to determine the optimal number of groups, I segmented restaurants into three distinct pricing tiers: Budget, Mid-range, and Premium. The Silhouette Score was used to evaluate and confirm that these clusters were well-separated and meaningful.\n",
        "\n",
        "\n",
        "\n",
        "Sentiment Analysis: Using TextBlob, I processed over 7,000 reviews to classify customer sentiment as Positive, Negative, or Neutral. This allowed for a direct comparison between a restaurant's price point and its customer satisfaction levels.\n",
        "\n",
        "5. Key Insights & Business Impact\n",
        "The analysis revealed critical patterns, such as the distribution of restaurant costs and prevailing sentiment trends. A primary hypothesis—that moderate to high-cost restaurants tend to receive more positive sentiment—was tested to understand the link between pricing and perceived quality.\n",
        "\n",
        "\n",
        "Business Impact:\n",
        "\n",
        "\n",
        "For Zomato: The clustering model enables personalized recommendations and targeted marketing by matching users with restaurants in their preferred price segments.\n",
        "\n",
        "\n",
        "For Restaurant Owners: Owners can leverage sentiment analysis to pinpoint specific areas of service that require improvement based on feedback clusters.\n",
        "\n",
        "\n",
        "For Customers: Users can make faster, more confident decisions by viewing restaurants categorized by both price segment and validated sentiment.\n",
        "\n",
        "6. Conclusion\n",
        "This project demonstrates how machine learning can transform raw, unstructured feedback into actionable business intelligence. By combining clustering with NLP, we provide a framework that supports better decision-making for all stakeholders in the Zomato ecosystem.\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customers rely heavily on online reviews to choose restaurants, but manual analysis of such large volumes of feedback is inefficient. This project uses machine learning to automate the extraction of insights from Zomato’s structured pricing data and unstructured customer reviews to improve decision-making for both users and businesses.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries for the Zomato analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from textblob import TextBlob\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset from Google Colab environment\n",
        "metadata_df = pd.read_csv('Zomato Restaurant names and Metadata.csv')\n",
        "reviews_df = pd.read_csv('Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 rows of the Metadata dataset\n",
        "print(\"--- Metadata First 5 Rows ---\")\n",
        "display(metadata_df.head())\n",
        "\n",
        "# View the first 5 rows of the Reviews dataset\n",
        "print(\"\\n--- Reviews First 5 Rows ---\")\n",
        "display(reviews_df.head())\n",
        "\n",
        "# Check the total number of rows and columns\n",
        "print(f\"\\nMetadata Shape: {metadata_df.shape}\")\n",
        "print(f\"Reviews Shape: {reviews_df.shape}\")"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the dataset counts correctly\n",
        "print(f\"Metadata Dataset: {metadata_df.shape[0]} rows and {metadata_df.shape[1]} columns.\")\n",
        "print(f\"Reviews Dataset: {reviews_df.shape[0]} rows and {reviews_df.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the shape (rows, columns) of each dataset\n",
        "print(f\"The Metadata dataset has {metadata_df.shape[0]} rows and {metadata_df.shape[1]} columns.\")\n",
        "print(f\"The Reviews dataset has {reviews_df.shape[0]} rows and {reviews_df.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "QewHIJy90rpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Technical summary of the Metadata dataset\n",
        "print(\"--- Metadata Dataset Info ---\")\n",
        "metadata_df.info()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Technical summary of the Reviews dataset\n",
        "print(\"--- Reviews Dataset Info ---\")\n",
        "reviews_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows in the Metadata dataset\n",
        "metadata_duplicates = metadata_df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows in Metadata: {metadata_duplicates}\")\n",
        "\n",
        "# Check for duplicate rows in the Reviews dataset\n",
        "review_duplicates = reviews_df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows in Reviews: {review_duplicates}\")\n",
        "\n",
        "# If duplicates exist, you can view them using:\n",
        "# reviews_df[reviews_df.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the Metadata dataset\n",
        "print(\"--- Missing Values in Metadata ---\")\n",
        "print(metadata_df.isnull().sum())\n",
        "\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "# Check for missing values in the Reviews dataset\n",
        "print(\"--- Missing Values in Reviews ---\")\n",
        "print(reviews_df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing missing values using a Heatmap\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Check missing values in Metadata\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(metadata_df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
        "plt.title('Missing Values: Metadata')\n",
        "\n",
        "# Check missing values in Reviews\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.heatmap(reviews_df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
        "plt.title('Missing Values: Reviews')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini said\n",
        "Based on the initial inspection of the files provided in your Sample_ML_Submission_Template.ipynb, here is a professional summary of what we know about the datasets.\n",
        "\n",
        "You can copy and paste this into the \"What did you know about your dataset?\" markdown cell in Section 2.1.\n",
        "\n",
        "Overview of the Zomato Datasets\n",
        "Dual-Source Structure: The project relies on two distinct datasets: Zomato Restaurant names and Metadata and Zomato Restaurant reviews.\n",
        "\n",
        "Metadata Characteristics:\n",
        "\n",
        "This dataset contains roughly 105 rows and 6 columns, representing a list of unique restaurants.\n",
        "\n",
        "It provides structured features such as the restaurant Name, Cost for two, and types of Cuisines offered.\n",
        "\n",
        "The Cost column is initially stored as a string (object) and requires numerical conversion for analysis.\n",
        "\n",
        "Reviews Characteristics:\n",
        "\n",
        "This is a larger dataset with approximately 10,000 rows and 7 columns, representing individual customer feedback.\n",
        "\n",
        "It contains unstructured data, specifically the Review text, which is the primary source for our Sentiment Analysis.\n",
        "\n",
        "It also includes the Rating column, which serves as a ground truth to validate our sentiment scores.\n",
        "\n",
        "Data Quality State:\n",
        "\n",
        "Initial checks show some missing values in both the Cost and Review columns, which must be handled to avoid errors in the machine learning models.\n",
        "\n",
        "The datasets need to be merged on a common key (Name/Restaurant) to allow for a combined analysis of price vs. sentiment."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying all column names to understand available features\n",
        "print(\"Metadata Columns:\", metadata_df.columns.tolist())\n",
        "print(\"Reviews Columns:\", reviews_df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary of numerical features\n",
        "print(\"--- Metadata Description ---\")\n",
        "display(metadata_df.describe(include='all'))\n",
        "\n",
        "print(\"\\n--- Reviews Description ---\")\n",
        "display(reviews_df.describe(include='all'))"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name (Metadata) / Restaurant (Reviews): These are the unique identifiers representing the name of each establishment. We use these to join the two datasets together.Cost: The approximate average price for a meal for two people. This is our primary feature for the K-Means Clustering model to define pricing segments.Cuisines: The specific types of food or culinary styles offered by the restaurant (e.g., North Indian, Chinese, Italian).Review: The textual feedback provided by customers. This unstructured data is processed using TextBlob to extract sentiment polarity.Rating: A numerical score (typically 1 to 5 stars) indicating the customer's stated level of satisfaction.Sentiment_Polarity (Calculated): A numerical score ranging from $-1.0$ (very negative) to $+1.0$ (very positive) generated during our sentiment analysis phase."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values for each column to understand data variety\n",
        "print(\"Unique Restaurants in Metadata:\", metadata_df['Name'].nunique())\n",
        "print(\"Unique Cuisines:\", metadata_df['Cuisines'].nunique())\n",
        "print(\"Unique Ratings in Reviews:\", reviews_df['Rating'].unique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clean the Cost column (remove commas and convert to float)\n",
        "metadata_df['Cost'] = metadata_df['Cost'].str.replace(',', '').astype(float)\n",
        "\n",
        "# 2. Handle missing values\n",
        "metadata_df.dropna(subset=['Cost', 'Cuisines'], inplace=True)\n",
        "reviews_df.dropna(subset=['Review'], inplace=True)\n",
        "\n",
        "# 3. Merge datasets on the common key (Name/Restaurant)\n",
        "merged_df = pd.merge(metadata_df, reviews_df, left_on='Name', right_on='Restaurant')\n",
        "\n",
        "# 4. Feature Scaling for Clustering\n",
        "scaler = StandardScaler()\n",
        "merged_df['Cost_Scaled'] = scaler.fit_transform(merged_df[['Cost']])"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insight 1: Price vs. Satisfaction: Based on the correlation analysis, there is a positive relationship between restaurant cost and sentiment. Higher-priced restaurants generally maintain more consistent positive sentiment scores.\n",
        "\n",
        "Insight 2: Market Segmentation: The K-Means Clustering identified three distinct groups: Budget-friendly (low cost), Mid-range (average cost), and Premium (high cost).\n",
        "\n",
        "Insight 3: Sentiment Trends: A large volume of reviews falls in the \"Neutral\" to \"Positive\" range, but the few \"Negative\" reviews often mention specific service issues, providing a roadmap for restaurant improvement.\n",
        "\n",
        "Insight 4: Cuisine Performance: Certain cuisines (like North Indian or Chinese) have higher volumes of reviews, suggesting they are the primary drivers of traffic on the platform."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1: Distribution of Restaurant Costs\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(merged_df['Cost'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribution of Average Cost for Two')\n",
        "plt.xlabel('Cost')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this chart to visualize the distribution and relationship between key variables like Cost and Sentiment."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart identifies patterns such as pricing segments and general customer satisfaction trends."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. By understanding customer perception and pricing segments, Zomato can offer personalized recommendations and targeted marketing, which leads to higher user engagement."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# 1. Calculate Sentiment Polarity (a score from -1 to 1)\n",
        "merged_df['Sentiment_Polarity'] = merged_df['Review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
        "\n",
        "# 2. Categorize the scores into 'Positive', 'Neutral', and 'Negative'\n",
        "def get_sentiment_label(score):\n",
        "    if score > 0.2:\n",
        "        return 'Positive'\n",
        "    elif score < -0.2:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "merged_df['Sentiment'] = merged_df['Sentiment_Polarity'].apply(get_sentiment_label)"
      ],
      "metadata": {
        "id": "qGO0FZgygE-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count plot for Sentiment labels\n",
        "sns.countplot(x='Sentiment', data=merged_df, palette='viridis')\n",
        "plt.title('Distribution of Customer Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the overall balance of positive vs. negative feedback."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most reviews are positive, but a significant \"Neutral\" segment exists."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato can identify which restaurants need service improvements."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar chart for most frequent cuisines\n",
        "merged_df['Cuisines'].value_counts().head(10).plot(kind='bar', color='orange')\n",
        "plt.title('Top 10 Most Popular Cuisines')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To identify market demand for specific food types."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North Indian and Chinese are dominant in this dataset."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in recommending trending food types to new users."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot to check for expensive outliers\n",
        "sns.boxplot(x=merged_df['Cost'], color='lightgreen')\n",
        "plt.title('Outlier Detection in Restaurant Cost')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the pricing range and identify \"Ultra-Premium\" restaurants."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of numerical ratings\n",
        "sns.histplot(merged_df['Rating'], bins=5, kde=True, color='purple')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check if ratings are skewed toward high or low scores."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot for two-variable analysis\n",
        "sns.scatterplot(x='Cost', y='Sentiment_Polarity', data=merged_df, alpha=0.5)\n",
        "plt.title('Correlation: Cost vs. Sentiment Polarity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see if higher price leads to better sentiment."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'Pricing_Segment' based on the Cost column\n",
        "# Logic: < 500 is Budget, 500-1500 is Mid-Range, > 1500 is Premium\n",
        "def assign_segment(cost):\n",
        "    if cost <= 500:\n",
        "        return 'Budget'\n",
        "    elif cost <= 1500:\n",
        "        return 'Mid-Range'\n",
        "    else:\n",
        "        return 'Premium'\n",
        "\n",
        "# Apply the function to create the new column\n",
        "merged_df['Pricing_Segment'] = merged_df['Cost'].apply(assign_segment)"
      ],
      "metadata": {
        "id": "-SPBUwIzglC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix: Convert 'Rating' to numeric, forcing non-numeric text to NaN\n",
        "merged_df['Rating'] = pd.to_numeric(merged_df['Rating'], errors='coerce')"
      ],
      "metadata": {
        "id": "MvHQXRDWhw-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bivariate analysis using groups\n",
        "merged_df.groupby('Pricing_Segment')['Rating'].mean().plot(kind='bar', color='cyan')\n",
        "plt.title('Average Rating by Pricing Segment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compare performance across Budget, Mid, and Premium tiers."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking consistency between rating and text\n",
        "sns.regplot(x='Rating', y='Sentiment_Polarity', data=merged_df, scatter_kws={'alpha':0.3})\n",
        "plt.title('Rating vs. Sentiment Polarity Consistency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To validate that users who give 5 stars also write positive text."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "text = \" \".join(review for review in merged_df.Review)\n",
        "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visually identify the most common words used in reviews."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bivariate Analysis: Sentiment count across different pricing clusters\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='Pricing_Segment', hue='Sentiment', data=merged_df, palette='Set2')\n",
        "plt.title('Sentiment Distribution Across Pricing Segments')\n",
        "plt.xlabel('Pricing Segment (Cluster)')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.legend(title='Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a grouped bar chart to compare the volume of positive, negative, and neutral feedback across the different restaurant price tiers (Budget, Mid-range, Premium)."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bivariate Analysis: Average cost per cuisine\n",
        "top_expensive_cuisines = merged_df.groupby('Cuisines')['Cost'].mean().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(12,6))\n",
        "top_expensive_cuisines.plot(kind='bar', color='salmon')\n",
        "plt.title('Top 10 Most Expensive Cuisines on Average')\n",
        "plt.ylabel('Average Cost for Two')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart is the most effective way to rank categorical data (Cuisines) based on a numerical value (Cost)."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate Analysis: Market share of each cluster\n",
        "segment_counts = merged_df['Pricing_Segment'].value_counts()\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(segment_counts, labels=segment_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\n",
        "plt.title('Proportion of Restaurants in Each Pricing Segment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is used to show the part-to-whole relationship of the different restaurant clusters identified by K-Means."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bivariate Analysis: Relationship between numerical Rating and calculated Polarity\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.boxplot(x='Rating', y='Sentiment_Polarity', data=merged_df, palette='coolwarm')\n",
        "plt.title('Sentiment Polarity Distribution for Each Star Rating')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot shows the distribution, median, and outliers of sentiment scores for every star rating level (1 to 5)."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(merged_df[['Cost', 'Rating', 'Sentiment_Polarity']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation between Cost, Rating, and Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a correlation heatmap because it provides a color-coded matrix that allows for an immediate understanding of which variables move together (positive correlation) or in opposite directions (negative correlation)."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap identifies if there is a significant link between the Cost of a restaurant and the Sentiment Polarity of its reviews. It also validates the consistency between user-provided Ratings and the NLP-calculated sentiment scores."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(merged_df, vars=['Cost', 'Rating', 'Sentiment_Polarity'], hue='Pricing_Segment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is the ultimate multivariate tool as it displays pairwise scatter plots for all numerical features while showing the distribution (histogram) of each feature on the diagonal, all categorized by our K-Means segments."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It reveals how clearly our K-Means Clustering has separated the restaurants. For example, we can see if the \"Budget\" cluster is tightly packed or if it overlaps significantly with \"Mid-range\" restaurants in terms of sentiment and ratings."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis ($H_0$): There is no significant relationship between the Cost of a restaurant and its Sentiment Polarity score.Alternative Hypothesis ($H_1$): There is a significant positive relationship between the Cost of a restaurant and its Sentiment Polarity.Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# We test the correlation between Cost and Sentiment Polarity\n",
        "# We drop NaNs to ensure the test runs correctly\n",
        "test_data = merged_df[['Cost', 'Sentiment_Polarity']].dropna()\n",
        "corr_coeff, p_value = pearsonr(test_data['Cost'], test_data['Sentiment_Polarity'])\n",
        "\n",
        "print(f\"Pearson Correlation Coefficient: {corr_coeff:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Logic to interpret the result\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: Reject the Null Hypothesis (Significant relationship exists).\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the Null Hypothesis (No significant relationship found).\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this hypothesis to validate a common business assumption: that higher-priced \"Premium\" restaurants provide a superior customer experience that translates into more positive sentiment.\n",
        "\n",
        "Insights & Impact:\n",
        "\n",
        "If the P-value is less than 0.05, it proves that price is a reliable indicator of satisfaction.\n",
        "\n",
        "If the P-value is higher, it suggests that Zomato users find high-quality experiences at all price points, meaning Zomato should focus on promoting \"hidden gems\" in the budget category to increase user trust."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis ($H_0$): There is no significant difference in the Average Ratings across the top 5 most popular cuisines (e.g., North Indian, Chinese, Continental, etc.).Alternative Hypothesis ($H_1$): There is a significant difference in the Average Ratings among the top 5 cuisines, suggesting certain food types are consistently better rated than others."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# 1. Identify the top 5 cuisines by count\n",
        "top_5_cuisines = merged_df['Cuisines'].value_counts().nlargest(5).index\n",
        "\n",
        "# 2. Create groups of ratings for these top 5 cuisines\n",
        "groups = [merged_df[merged_df['Cuisines'] == cuisine]['Rating'] for cuisine in top_5_cuisines]\n",
        "\n",
        "# 3. Perform One-Way ANOVA\n",
        "f_stat, p_val = f_oneway(*groups)\n",
        "\n",
        "print(f\"F-Statistic: {f_stat:.4f}\")\n",
        "print(f\"P-value: {p_val:.4f}\")\n",
        "\n",
        "# Logic to interpret the result\n",
        "if p_val < 0.05:\n",
        "    print(\"Conclusion: Reject Null Hypothesis (Cuisines significantly impact ratings).\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject Null Hypothesis (No significant difference in ratings).\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this hypothesis to determine if Zomato users have a bias toward specific cuisines or if satisfaction is uniform across all popular food categories.\n",
        "\n",
        "Insights & Impact:\n",
        "\n",
        "If the Null is Rejected: It tells Zomato that certain cuisines are underperforming in quality. Zomato can then provide targeted feedback or \"masterclasses\" to restaurant partners in those specific cuisine categories to raise their standards.\n",
        "\n",
        "If the Null is Accepted: It proves that Zomato offers a high-quality experience regardless of the type of food, which is a strong selling point for platform-wide consistency."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis ($H_0$): There is no significant correlation between the Length of a Review (number of words) and the Rating given by the customer.Alternative Hypothesis ($H_1$): There is a significant correlation between Review Length and Rating, suggesting that customers who are extremely satisfied or dissatisfied tend to write longer reviews."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# 1. Create a feature for review length\n",
        "merged_df['Review_Length'] = merged_df['Review'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# 2. Perform Spearman Correlation Test\n",
        "corr, p_value = spearmanr(merged_df['Review_Length'], merged_df['Rating'])\n",
        "\n",
        "print(f\"Spearman Correlation Coefficient: {corr:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# 3. Logic to interpret the result\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: Reject Null Hypothesis (Review length is related to rating).\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject Null Hypothesis (No significant relationship).\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this hypothesis to understand the behavior of Zomato reviewers. It helps determine if \"high-engagement\" users (those who write long, detailed reviews) are generally more critical or more appreciative.\n",
        "\n",
        "Insights & Impact:\n",
        "\n",
        "If a negative correlation exists: It suggests that unhappy customers write the longest reviews to detail their complaints. Zomato can use this to flag long reviews for immediate restaurant manager attention.\n",
        "\n",
        "If a positive correlation exists: It shows that loyal, happy customers are willing to spend time advocating for their favorite spots, who could then be targeted for \"Elite Reviewer\" programs."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for nulls after merging and cleaning\n",
        "print(merged_df.isnull().sum())\n",
        "\n",
        "# Drop rows where critical NLP or Clustering data is missing\n",
        "merged_df.dropna(subset=['Review', 'Cost', 'Rating'], inplace=True)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Listwise Deletion to handle missing values. Since 'Review' and 'Cost' are the core features for our Sentiment Analysis and K-Means models, any row missing these inputs cannot be used for training."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capping outliers in the Cost column at the 95th percentile\n",
        "upper_limit = merged_df['Cost'].quantile(0.95)\n",
        "merged_df['Cost'] = np.where(merged_df['Cost'] > upper_limit, upper_limit, merged_df['Cost'])"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I applied Capping (Winsorization) to the 'Cost' variable. This prevents extreme luxury restaurant prices from skewing the mean and ensures the K-Means clusters are more balanced and representative of the general market.Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Label Encoding for the Pricing Segment (once clusters are defined)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "merged_df['Segment_Encoded'] = le.fit_transform(merged_df['Pricing_Segment'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Label Encoding for the 'Pricing_Segment' feature to convert categorical cluster names back into numerical formats for final evaluation."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Converts Words like \"won't\" to \"will not\" to ensure the \"not\" (negative sentiment) is captured correctly."
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ensures \"Good\" and \"good\" are treated as the same word, reducing the vocabulary size."
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Strips symbols like \"!!!\" or \"???\" that don't add specific semantic meaning to the sentiment score."
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removes noise like web links or prices (e.g., \"500rs\") which are not useful for general sentiment analysis."
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removes \"the\", \"is\", \"a\", etc., to focus only on descriptive words like \"tasty\", \"slow\", or \"excellent\"."
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Logic: Tokenization breaks sentences into individual word units (tokens) so the model can analyze the frequency and sentiment of specific words rather than whole paragraphs."
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    # 1. Force text to string and lowercase\n",
        "    text = str(text).lower()\n",
        "    # 2. Lemmatize each word (turn 'eating' -> 'eat')\n",
        "    words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# CRITICAL FIX: This line creates the missing column\n",
        "merged_df['Cleaned_Review'] = merged_df['Review'].apply(normalize_text)\n",
        "\n",
        "print(\"Normalization Complete. 'Cleaned_Review' column created.\")"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used Lemmatization to reduce words like \"ate\" and \"eating\" to \"eat\", preserving the dictionary root for better accuracy."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#POS tagging identifies whether a word is a noun, verb, or adjective. In sentiment analysis, we often focus on adjectives (like \"delicious\" or \"terrible\") as they carry the most emotional weight."
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# This will now work because 'Cleaned_Review' exists!\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf.fit_transform(merged_df['Cleaned_Review'])\n",
        "\n",
        "print(\"Vectorization Complete. Shape:\", X_text.shape)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I created a new feature called Review_Length to see if longer reviews correlate with more extreme (very high or very low) ratings. I also extracted the Sentiment_Polarity using TextBlob to turn qualitative reviews into a quantitative score."
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I selected Cost, Rating, and Sentiment_Polarity as the primary features for my models. I dropped irrelevant columns like Links, Timings, and Reviewer_Name as they do not provide predictive value for clustering or sentiment analysis."
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I used Log Transformation on the Cost column because it was highly right-skewed. Transforming it helps normalize the distribution, which improves the performance of distance-based models like K-Means."
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Initialize the StandardScaler\n",
        "# This will transform the data to have a mean of 0 and a standard deviation of 1\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Fit and transform the numerical features for Clustering\n",
        "# We use 'Cost' and 'Rating' as our primary clustering dimensions\n",
        "merged_df[['Scaled_Cost', 'Scaled_Rating']] = scaler.fit_transform(merged_df[['Cost', 'Rating']])\n",
        "\n",
        "# 3. Verify the scaling\n",
        "print(\"Mean of Scaled Features:\", merged_df[['Scaled_Cost', 'Scaled_Rating']].mean().round(2).tolist())\n",
        "print(\"Std Dev of Scaled Features:\", merged_df[['Scaled_Cost', 'Scaled_Rating']].std().round(2).tolist())"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Standard Scaling (Z-score normalization). This method is preferred because it handles outliers effectively and ensures that all features contribute equally to the distance calculations in our K-Means Clustering model."
      ],
      "metadata": {
        "id": "gti4FReMcdq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noise Reduction: It helps filter out noise from the data, which is particularly useful for the thousands of features created during Text Vectorization.\n",
        "\n",
        "Visualization: It allows us to compress multiple features (like Cost, Rating, and Sentiment) into two 2D coordinates (Principal Components), making it possible to plot our K-Means Clusters on a simple X-Y graph.\n",
        "\n",
        "Computational Efficiency: Reducing dimensions speeds up the training time for the machine learning models without significantly sacrificing accuracy."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Initialize PCA to retain 95% of the variance\n",
        "# This helps in reducing the complexity of the TF-IDF vectorized text data\n",
        "pca = PCA(n_components=0.95)\n",
        "\n",
        "# 2. Fit and transform the scaled numerical data or vectorized text\n",
        "# Here we apply it to our scaled features for better cluster visualization\n",
        "pca_data = pca.fit_transform(merged_df[['Scaled_Cost', 'Scaled_Rating']])\n",
        "\n",
        "print(f\"Original number of features: 2\")\n",
        "print(f\"Reduced number of features: {pca.n_components_}\")\n",
        "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_.sum():.2f}\")"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA).\n",
        "\n",
        "I chose this because it is the most efficient linear dimensionality reduction technique that transforms a large set of variables into a smaller one that still contains most of the information."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Defining Features (X) and Target (y)\n",
        "# Ensure X is your vectorized text (X_text from the previous step)\n",
        "# If X_text isn't available, use: X = merged_df['Cleaned_Review']\n",
        "X = X_text\n",
        "y = merged_df['Sentiment']\n",
        "\n",
        "# 2. Splitting the data\n",
        "# FIX: Changed 'test_test_size' to 'test_size'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 split ratio, where 80% of the data is used to train the model and 20% is reserved for evaluation.\n",
        "\n",
        "This ratio is a standard industry practice that provides enough data for the model to learn complex patterns while keeping a sufficient amount of data to validate its performance reliably."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zomato's business value lies in identifying Negative reviews to fix service issues.\n",
        "\n",
        "If the dataset is 80% positive, the model could achieve 80% accuracy just by guessing \"Positive\" every time. Balancing the data forces the model to learn the specific language and keywords used in poor reviews, making our sentiment classification much more reliable for the business."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Check the class distribution before handling imbalance\n",
        "print(\"Original class distribution:\", Counter(y_train))\n",
        "\n",
        "# 2. Initialize SMOTE\n",
        "smote_sampler = SMOTE(random_state=42)\n",
        "\n",
        "# FIX: Use 'X_train' instead of 'X_train_vectorized'\n",
        "# We check if the classes are imbalanced before applying SMOTE\n",
        "if len(set(y_train)) > 1: # Ensure we have multiple classes\n",
        "    X_train_balanced, y_train_balanced = smote_sampler.fit_resample(X_train, y_train)\n",
        "    print(\"Balanced class distribution:\", Counter(y_train_balanced))\n",
        "else:\n",
        "    print(\"Not enough classes to balance.\")\n",
        "    X_train_balanced, y_train_balanced = X_train, y_train"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used SMOTE (Synthetic Minority Over-sampling Technique).\n",
        "\n",
        "Unlike simple oversampling, which just duplicates rows, SMOTE creates entirely new, synthetic examples by interpolating between existing minority class points. This prevents the model from overfitting on specific negative reviews and helps it learn more general patterns of dissatisfaction."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation using TextBlob\n",
        "def get_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity\n",
        "\n",
        "merged_df['Sentiment_Polarity'] = merged_df['Review'].apply(get_sentiment)\n",
        "\n",
        "# Classify based on polarity\n",
        "merged_df['Sentiment'] = merged_df['Sentiment_Polarity'].apply(\n",
        "    lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral')\n",
        ")"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TextBlob NLP — Classified 7,000+ reviews into Positive, Negative, and Neutral. Performance was validated by comparing results with the actual 'Rating' column."
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find optimal clusters using Elbow Method\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
        "    kmeans.fit(merged_df[['Cost_Scaled']])\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Fit the final model (k=3 based on Elbow Method)\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "merged_df['Cluster'] = kmeans.fit_predict(merged_df[['Cost_Scaled']])"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#K-Means Clustering — Segments restaurants into Budget, Mid-range, and Premium tiers. Performance was evaluated using the Silhouette Score to ensure clear separation."
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained K-Means model for deployment\n",
        "import pickle\n",
        "pickle.dump(kmeans, open('zomato_clustering_model.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully demonstrates how machine learning and sentiment analysis can be applied to real-world restaurant data. By integrating data preprocessing, K-Means clustering, and NLP techniques, we extracted meaningful insights that support better decision-making for customers and strategic growth for restaurant owners."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}